{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eysqzMhpPkSj"
      },
      "outputs": [],
      "source": [
        "# installing spark in colab and creating spark session\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "class AirportPageRank:\n",
        "        \n",
        "    def __init__(self, inputFileLink, maxIter, alpha = 0.15):\n",
        "        self.inputFileLink = inputFileLink\n",
        "        self.maxIter = maxIter\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.loadFile()\n",
        "        self.getNodeCount()\n",
        "        self.getDanglingNodes()\n",
        "        \n",
        "        self.createNodePair()\n",
        "    \n",
        "    def loadFile(self):\n",
        "        spark.sparkContext.addFile(self.inputFileLink)\n",
        "        self.flightsDf = spark.read.csv(\"file://\"+SparkFiles.get(\"Airports_Jan_2022.csv\"), header = True, inferSchema = True)\n",
        "        \n",
        "    def getNodeCount(self):\n",
        "        self.origin = self.flightsDf.select(['ORIGIN']).distinct()\n",
        "        self.dest = self.flightsDf.select(['DEST']).distinct()\n",
        "        totalNodes = self.origin.union(self.dest).distinct()\n",
        "        self.N = totalNodes.count()\n",
        "    \n",
        "    def getDanglingNodes(self):\n",
        "        dang = self.dest.subtract(self.origin)\n",
        "        self.danglingNodes = dang.rdd.map(lambda x : (x.DEST, \"no_dest\"))\n",
        "    \n",
        "    def createNodePair(self):\n",
        "        flights = self.flightsDf.select(['ORIGIN', 'DEST'])\n",
        "        flightsRdd = flights.rdd\n",
        "        # creating source destination pair\n",
        "        self.flightSrcDest = flightsRdd.map(lambda row : (row.ORIGIN, row.DEST)) # maps over rdd\n",
        "        \n",
        "    def calculatePageRank(self):\n",
        "        \n",
        "        flightPageRank = self.flightSrcDest.map(lambda x : (x[0], (x[1], 10)))\n",
        "        \n",
        "        for iter in range(self.maxIter):\n",
        "            \n",
        "            # this represents the number of outgoing links for each node\n",
        "            flightEdges = flightPageRank.map(lambda x : (x[0], 1)).reduceByKey(lambda x, y : x+y)\n",
        "            \n",
        "            rankEdge = flightPageRank.join(flightEdges)\n",
        "            \n",
        "            # page rank\n",
        "            pageRank = rankEdge.map(lambda x : (x[1][0][0], x[1][0][1]/x[1][1])).reduceByKey(lambda x, y: x+y)\n",
        "            \n",
        "            # calculating sum of pagerank values of dangling nodes\n",
        "            mass = sc.accumulator(0)\n",
        "            pageRank.join(self.danglingNodes).foreach(lambda x : mass.add(x[1][0]))\n",
        "            dangPageRank = pageRank.join(self.danglingNodes)\n",
        "            dangMass = mass.value\n",
        "            \n",
        "            # calculating final PageRank value of this iteration\n",
        "            G = self.N\n",
        "            alphaTemp = self.alpha\n",
        "            # we cannot use class variables directly in lambda function\n",
        "            pageRank = pageRank.map(lambda x : (x[0], x[1]+(dangMass/G)))\n",
        "            pageRank = pageRank.map(lambda x : (x[0], x[1]*(1-alphaTemp)))\n",
        "            pageRank = pageRank.map(lambda x : (x[0], x[1]+(alphaTemp/G)))\n",
        "    \n",
        "            # replacing old page rank with new calculated page rank value\n",
        "            flightPageRank = self.flightSrcDest.join(pageRank)\n",
        "        \n",
        "        # final calculated page after running all iterations\n",
        "        finalPageRank = pageRank.sortBy(lambda x : -x[1])\n",
        "        return finalPageRank"
      ],
      "metadata": {
        "id": "BSqYD_Y0PqTx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetUrl = \"/content/Airports_Jan_2022.csv\"\n",
        "result = AirportPageRank(datasetUrl, 10)\n",
        "outputRdd = result.calculatePageRank()\n",
        "schema = [\"airport_node\", \"page_rank_value\"]\n",
        "outputDf = spark.createDataFrame(outputRdd, schema)"
      ],
      "metadata": {
        "id": "MMkf7LFCP9-b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputDf.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl7KL-T3Qhuq",
        "outputId": "f5bc58fd-d787-4771-d7f6-27f271aa728b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(airport_node='ORD', page_rank_value=47.250122427267016),\n",
              " Row(airport_node='DEN', page_rank_value=43.218340813925046),\n",
              " Row(airport_node='ATL', page_rank_value=37.34263691741187),\n",
              " Row(airport_node='ANC', page_rank_value=31.682317826385457),\n",
              " Row(airport_node='LAS', page_rank_value=29.669637219743297),\n",
              " Row(airport_node='LAX', page_rank_value=29.210093861252542),\n",
              " Row(airport_node='DFW', page_rank_value=28.71073282289035),\n",
              " Row(airport_node='IAH', page_rank_value=28.068463479449694),\n",
              " Row(airport_node='PHX', page_rank_value=26.550432320607655),\n",
              " Row(airport_node='BET', page_rank_value=25.511383054450203)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}